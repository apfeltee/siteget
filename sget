#!/usr/bin/ruby

require "uri"
require "json"
require "yaml"
require "digest"
require "fileutils"
require "net/https"
require "nokogiri"
require "trollop"

USER_AGENT = "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.80 Safari/537.36"

class SiteGet
  @@selectors =
  [
    # javascript
    {sel: "script[src]", rep: "src", ext: "js", post: nil}, #JSPostProcessor
    # images
    {sel: "img", rep: "src", ext: nil},
    {sel: "img[data-thumb]", rep: "data-thumb", ext: nil, post: nil},
    {sel: "img[data-src]", rep: "data-src", ext: nil, post: nil},
    {sel: "input[src][type=image]", rep: "src", ext: nil, post: nil},
    # css
    {sel: "link[rel=stylesheet][href]", rep: "href", ext: "css", post: nil}, # CSSPostProcessor
    # favicons
    {sel: "link[rel*=icon][href]", rep: "href", ext: nil, post: nil},
    {sel: "link[rel*=shortcut][href]", rep: "href", ext: nil, post: nil},
  ]

  def initialize(url, opts)
    $stdout.sync = false
    @opts = opts
    @site = geturl(url)
    @parsed = URI.parse(url)
    @flog = {"info" => {}, "urls" => []}
  end

  def mlog(str)
    $stdout << "-- " << str << "\n"
  end

  def writelog
    File.open(File.join(@opts[:resdir], "url-log.json"), "wb") do |fh|
      fh << JSON.pretty_generate(@flog)
    end
  end

  def geturl(url, **kwargs)
    mlog "downloading #{url.inspect} ..."
    uri = URI.parse(url)
    http = Net::HTTP.new(uri.host, uri.port)
    if url.start_with?("https") then
      http.use_ssl = true
    end
    http.verify_mode = OpenSSL::SSL::VERIFY_NONE
    request = Net::HTTP::Get.new(uri.request_uri, {'User-Agent' => USER_AGENT})
    response = http.request(request)
    return response
  end

  def mkname(url, selector)
    # get url components (for query string)
    purl = URI.parse(url)
    extension = File.extname(purl.path)
    basename = File.basename(purl.path, extension)
    # make a "clean" name...
    # well, not really clean, but better than using a sha256 hash
    cleanname = (basename + (purl.query ? purl.query : "")).gsub(/[^0-9A-Za-z]/, "-").gsub(/^-/, "")
    # if filename exceeds a certain length, turn it into a hash sum instead
    # (rather than trying to figure out how to create a unique filename, even by shrinking ...)
    # appends the first 10 characters from the original cleanname, to ease identification
    if cleanname.length > 500 then
      mlog "filename is too long, using hashsum instead"
      md5 = Digest::MD5.new
      md5.update cleanname
      cleanname = cleanname[0 .. 10] + md5.hexdigest 
    end
    # create a physical location for the file to be written to
    if (selector[:ext] == nil) && (extension.length == 0) then
      extension = "unknown"
    end
    if extension.length > 0 then
      extension = extension[1 .. -1]
    end
    return cleanname + "." + extension
  end

  def mkurl(oldurl, selector)
    url = oldurl
    if oldurl.match(/^(https?|ftp)/) or (autoscm = oldurl.match(/^\/\//)) then
      if autoscm then
        # in case of "//somehost.com/..." urls, just create an absolute url by
        # reusing the scheme from the mothership
        url = @parsed.scheme + ":" + oldurl
      end
    else
      # otherwise, macgyver the url together from the mothership scheme and host
      url = String.new
      url << @parsed.scheme << "://" << @parsed.host
      # if the url starts with a slash, it's an absolute path (i.e., "/blah/something.js")
      if oldurl.match(/^\//) then
        url << oldurl
      else
        # otherwise, it's a relative one (i.e., "subdir/whatever/thing.js")
        url << File.dirname(@parsed.path) << "/" << oldurl
      end
    end
    fulldest = File.join(@opts[:resdir], mkname(url, selector))
    # coincidentally, the local url is the same as the file dest. whudathunkit
    @flog["urls"] << {"url" => url, "local" => fulldest}
    if File.file?(fulldest) then
      return fulldest
    end
    response = geturl(url)
    if response then
      FileUtils.mkdir_p(@opts[:resdir])
      File.open(fulldest, "wb") do |fh|
        fh << response.body
      end
      return fulldest
    end
    raise Exception, "bad response from geturl?"
  end

  def parse
    @flog["info"]["mainpage"] = @parsed.to_s
    body = @site.body
    main = Nokogiri::HTML(body)
    @@selectors.each do |selector|
      # iterate over each selector
      mlog "++ processing selector #{selector[:sel].inspect} ..."
      if (nodes = main.css(selector[:sel])) then
        nodes.each do |node|
          url = node[selector[:rep]]
          localurl = mkurl(url, selector)
          mlog "rewriting #{url.inspect}"
          # modify the node with the new url
          # gotta love nokogiri
          node[selector[:rep]] = localurl
        end
      end
    end
    # write the shite to file
    File.open(@opts[:htmlfile], "w") do |fh|
      fh << main.to_html
    end
  end
end

opts = Trollop::options do
  opt :destination, "Use <destination> as directory to store website", default: ".", type: :string
  opt :htmlfile, "Use <htmlfile> instead of index.html", default: "index.html", type: :string
  opt :resdir, "Use <resdir> instead of 'res' as resources directory name", default: "res", type: :string
end

argv = ARGV
if argv.length > 0 then
  sg = SiteGet.new(argv.shift, opts)
  begin
    sg.parse
  ensure
    sg.writelog
  end
end
